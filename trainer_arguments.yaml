# MoE specific arguments
monitored_layers: all
router_loss_coef: 0.001

# Custom training arguments
tokenizer: "HuggingFaceTB/SmolLM2-1.7B"
xlstm_config_path: "./model_config.yaml"
dataset_url: "HuggingFaceTB/smollm-corpus"
features: ["text"]

train_subset: "cosmopedia-v2"
train_split: "train"
train_samples: 5000

eval_subset: "fineweb-edu-dedup"
eval_split: "train"
eval_samples: 1000

# HF TrainerArguments arguments
num_train_epochs: 4
gradient_accumulation_steps: 4
per_device_train_batch_size: 4
per_device_eval_batch_size: 4

# optim args
learning_rate: 0.0002
weight_decay: 0.001
warmup_ratio: 0.1
optim: "adamw_torch"
lr_scheduler_type: "cosine"
torch_compile: false
fp16: true
use_cpu: false
gradient_checkpointing: false

output_dir: "artifacts"
logging_dir: "artifacts"
report_to: "tensorboard"
logging_steps: 100
save_steps: 100
hub_private_repo: false
save_total_limit: 2
load_best_model_at_end: true
eval_strategy: "steps"

push_to_hub: true
resume_from_checkpoint: "artifacts"
hub_model_id: "thiomajid/moe-xlstm"
remove_unused_columns: false
